{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "068a2db2-5f78-4664-a1e2-2ca5c7c73057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a47a435f-8c02-4b42-81ce-4c234d233feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26dd5db0-cd4e-4d89-999d-40ffb4f162eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb8f7323-f432-439a-b3ff-a1dd7d4a80c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff3ff6fb-7116-4e66-a2d0-0bf89904a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GD we use SGDRegressor\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db99ffb-6e71-4de9-9a44-28251d47ed17",
   "metadata": {},
   "source": [
    "# Parameters:\n",
    "### **`2.1.penalty='l2':`**\\\n",
    "This specifies the regularization technique being used. The 'l2' penalty refers to Ridge regression (also known as Tikhonov regularization), where the model tries to minimize the sum of squared weights (L2 norm of the weights).\\\n",
    "This helps in preventing overfitting by penalizing large coefficients, thereby introducing some bias but reducing variance.\\\n",
    "**`2.2. max_iter=500:`**\n",
    "This defines the maximum number of iterations the model will go through while updating the weights using stochastic gradient descent.\\\n",
    "In this case, the model will run up to 500 iterations, which means the algorithm will go over the training data up to 500 times.\\\n",
    "**`2.3. eta0=0.1:`**\n",
    "This is the initial learning rate. It controls how big of a step the algorithm takes in the direction of the negative gradient during weight updates.\\\n",
    "A higher learning rate means larger steps, which can speed up convergence but may risk overshooting the optimal solution. A lower learning rate means smaller steps, which might make convergence slower but more stable.\\\n",
    "In this case, the initial learning rate is 0.1, which is a moderately fast step size.\\\n",
    "**`2.4. learning_rate='constant':`**\n",
    "This controls how the learning rate changes during the training process.\\\n",
    "'constant' means the learning rate will stay the same (i.e., eta0 = 0.1) throughout all iterations.\\\n",
    "There are other options like 'optimal', 'invscaling', and 'adaptive', where the learning rate changes over time (e.g., decreases as training progresses).\\\n",
    "**`2.5. alpha=0.001:`**\n",
    "This is the regularization strength for the L2 penalty (ridge regularization).\\\n",
    "The parameter alpha determines the amount of regularization applied to the model. A larger alpha applies stronger regularization, shrinking the model coefficients more, whereas a smaller alpha applies weaker regularization.\\\n",
    "In this case, alpha=0.001 applies light regularization, so the model is only slightly penalized for large weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce881e00-ad90-455f-8a4d-a85f93734613",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = SGDRegressor(penalty='l2',max_iter=500,eta0=0.1,learning_rate='constant',alpha=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f2ea5f9-07f6-48e2-ac7b-e8e1df748045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score 0.39052713215255985\n",
      "[  49.57361281 -167.30249902  375.17973247  275.0106658   -10.32405151\n",
      "  -61.12919972 -168.88278633  138.52355946  336.95575794   90.2639166 ]\n",
      "[136.6804212]\n"
     ]
    }
   ],
   "source": [
    "reg.fit(X_train,y_train)\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "print(\"R2 score\",r2_score(y_test,y_pred))\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "104aaabc-9788-4629-bc90-9b9d2f3d118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da46f0-5ee3-4e72-8cd2-d84207f3eb3f",
   "metadata": {},
   "source": [
    "**Traditional solvers like 'cholesky' or 'svd' may not be efficient for large sparse datasets because they require more memory to store dense matrices or perform direct matrix inversion.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad281762-8980-4684-98c8-9135de10c453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\"\"\" A small alpha like 0.001 means only slight regularization is applied,\n",
    "which helps prevent overfitting while still allowing flexibility to fit the data.\n",
    "\n",
    "The sparse_cg solver in Ridge regression is used primarily when dealing with large,\n",
    "sparse datasets\"\"\"\n",
    "reg = Ridge(alpha=0.001, max_iter=500,solver='sparse_cg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f34ef255-47dc-43db-85cd-aae834b7be40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score 0.4625010162027918\n",
      "[  34.52192778 -290.84083871  482.40181675  368.06786931 -852.44872818\n",
      "  501.59160694  180.11115474  270.76334443  759.73534802   37.49135796]\n",
      "151.101985182554\n"
     ]
    }
   ],
   "source": [
    "reg.fit(X_train,y_train)\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "print(\"R2 score\",r2_score(y_test,y_pred))\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da970fae-f30a-4188-ad15-d5f373fc80e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
